---
title: "Chapter XXX -- Soccer"
author: "Ryan Elmore"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r knitr-options, include = FALSE}
knitr::opts_chunk$set(fig.align="center",
                      warning = FALSE,
                      message = FALSE,
                      comment = NA)
```

```{r, eval=F, include = F, message = F}
install.packages("ISAR")
```

```{r libs, include=T, message = F, warning = F}
library(dplyr)
library(ggplot2)
library(lubridate)
library(rvest)
library(janitor)
library(scales)
library(readr)
library(ggrepel)
library(ggdendro)
library(ISAR)
library(mixtools)
library(factoextra)
library(worldfootballR)
theme_set(theme_bw())
```

# Soccer Data

In this chapter, we will explore several unsupervised learning methods, e.g. 
hierarchical clustering, using several hockey data sets. 

## FB Reference 

In October 2022, the Sports Reference website announced a partnership with 
Stats Perform Opta to dramatically increase the quantity and quality of soccer, 
or football, data that are available on its site. Soccer data are now hosted at
https://fbref.com/en/ and includes data on most major professional leagues 
throughout the world at a team and player level. More details related to this 
new partnership can be found here. 

https://www.sports-reference.com/blog/2022/10/fbref-leagues-ðŸ‡µðŸ‡¹-leagues-ðŸ‡§ðŸ‡·-leagues-ðŸ‡²ðŸ‡½-expanded-womens-and-mens-data-new-data-partner/

In this chapter, we will focus on using soccer data to cluster both teams and 
individual players using data from fbref.com. 

## Expected Goals and Assists

Similar to hockey, soccer is notoriously low-scoring and there is 
certainly an element of luck in scoring a goal. Therefore, one of the main 
developments in the soccer-analytics space is the introduction of Expected Goals, 
or xG for short. There are various models to compute xG, but at a high level xG is
simply the probability of scoring a goal from a given spot on the field when a 
shot is taken.

Given that most shots do not result in a goal, most passes to a shot taker will 
likely lead to a missed goal as well. Therefore, there is an analogous result for 
assists that is referred to as Expected Assists, or xA for short. You will often
see these metrics normalized to ninety minutes so that a player is a substitute 
and only plays thirty minutes can be directly compared to a starter who might 
play sixty - ninety minutes. The normalized expected goals and assists are often
written as xG (xA) per ninety. 

## Player Statistics

We will initially focus our attention on clustering individual players from the 
American NWSL. The NWSL is National Women's Soccer League and is the largest
women's soccer league in North America. In the 2022 season there were twelve teams
and the Portland Thorns were crowned champion. 

We downloaded a csv file of the NWSL player statistics
from the following fbref.com website and the data are also given in the book's R 
package as `nwsl_player_stats`. Note that the column, or variable, names are defined 
on the Football Reference website. https://fbref.com/en/comps/182/stats/NWSL-Stats


Here's how the data were obtained. Note, they are now located in the `ISAR` package
under the `nwsl_player_stats` object. 

```{r, eval = F}
nwsl_player_stats <- read.csv("../data/nwsl-players.csv", header = T) |> 
  clean_names() |> 
  select(c(2:6, 8:10, 28:32)) |> 
  rename(xGp90 = x_g_1,
         xAp90 = x_ag_1,
         xGxAp90 = x_g_x_ag,
         npxGp90 = npx_g_1,
         npxGxAp90 = npx_g_x_ag_1)
saveRDS(nwsl_player_stats, "../data/nwsl_player_stats.rds")
```

### Exploratory Data Analysis

We will begin by exploring expected goals. In particular, let's look at xG by 
age in the NWSL. We will use the `xGp90` column as our xG value because it is 
the column representing the normalized xG value, normalizing xG per 90 minutes 
played. In addition, we restrict our analysis to the non-goalkeeping position
players as the goalkeepers are not likely to score or assist on a goal. 

```{r}
p <- ggplot(data = nwsl_player_stats |> 
              filter(pos != "GK"),
            aes(x = age, y = xGp90))
p + geom_point(alpha = .5) +
  labs(x = "age", y = "expected goals (xG)") +
  theme_bw()
```
One thing that we might want to do in plotting sports data is to identify the 
players that we are plotting. In this case, we can use the `ggrepel` package to 
quickly add labels to certain points using the `label` aesthetic and the
`geom_text_repel()` function. Suppose we want to identify players who are less 
than 20 years of age with a positive xG value, or have an xG value greater than 
0.6. 

```{r}
p <- ggplot(data = nwsl_player_stats |> 
              filter(pos != "GK") |> 
              mutate(new_player = ifelse((age < 20 & xGp90 > 0) | xGp90 > 0.6,
                                         player, "")),
            aes(x = age, y = xGp90, label = new_player))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals (xG)") +
  geom_text_repel() +
  theme_bw()
```

## Clustering

In this chapter we will focus on several analytical tools that fall under the  
statistical, or machine, learning umbrella of unsupervised learning. Unsupervised
learning is generally thought to be exploratory data analytic techniques that 
are designed to learn as much as possible about the data without the aid of 
labels as is the case in a regression or classification model (supervised 
learning). Clustering, in particular, refers to the class of techniques concerned
with finding groups within your data set that are similar (in some sense) with 
respect to the measured variables (or covariates). 

### $K$-Means

In short, $K$-means clustering refers to assigning each observation to one of 
$K$ clusters. The number of clusters, $K$ is specified *a priori*. The algorithm
for computing a $K$-means clustering of $n$ observations is relatively simple 
and intuitive. Each observation is randomly assigned to one of the $K$ clusters
and the "center" of each cluster is computed, e.g. the mean vector in $K$-means. 
Next, you assign each observation to the closest "center", recompute the "center", 
and reassign each observation until observations stop being reassigned. 

#### Clustering with Age

First let's look at a clustering based on age of the player (age), total minutes 
played in the season (min), and expected goals and assists per ninety minutes 
played (xGxAp90). We are going to consider three clusters initially. 

```{r}
nwsl_cluster_df <- nwsl_player_stats |> 
  mutate(player_team = paste(player, squad, sep = "_")) |> 
  select(player, squad, player_team, pos, age, min, starts, xGp90, xAp90, 
         xGxAp90, npxGp90, npxGxAp90) |> 
  filter(xGxAp90 < 1.5, pos != "GK") |> 
  na.omit()

set.seed(9019)
km <- kmeans(select(nwsl_cluster_df, age, min, xGp90, xAp90) |> 
               na.omit(),
             centers = 3)
```

How can we visualize these clusters? Let's plot age versus expected goals and 
assists, and color these points by their clusters. 

```{r}
nwsl_cluster_df <- mutate(nwsl_cluster_df, k_means = km$cluster)

p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = min, col = as.factor(k_means)))
p + geom_point() +
  scale_color_grey("cluster") +
  labs(x = "Age of Player",
       y = "Minutes Played")
```

What stands out amongst this clustering of points? Clearly the clusters are 
dominated by different age strata. It looks like the first cluster is composed
of players that played more than approximately 1200 minutes, the second contains
players playing (roughly) 500 to 1200 minutes, and finally the last cluster 
contains players playing less than 500 minutes. 

Is anything else evident if we omit minutes played and plot age of the player 
versus expected goals and assists? 

```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = xGxAp90, col = as.factor(k_means)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
  labs(x = "Age of Player",
       y = "Expected Goals and Assists per 90")
  

```

#### Remove Minutes Played

Given that minutes played dominates the previous clustering, consider a 
$K$-means clustering age, expected goals, and expected assists, both normalized
by per ninety minutes played. 

```{r}
set.seed(9019)
km <- kmeans(select(nwsl_cluster_df, age, xGp90, xAp90) |> 
               na.omit(),
             centers = 3)

nwsl_cluster_df <- mutate(nwsl_cluster_df, clusters = km$cluster)
```

```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = xGp90, col = as.factor(clusters)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
    labs(x = "Age of Player",
       y = "Expected Goals per 90")

```

As you might imagine, the clustering is now dominated by the age variable.  


```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = xAp90, col = as.factor(clusters)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
    labs(x = "Age of Player",
       y = "Expected Assists per 90")

```

It's obvious that by including `age` and `minutes` seems to dominate
the makeup of the clusters. So, if we want to identify similar players 
based on their scoring ability, let's only consider the appropriate metrics. Here
we separate the expected goals and assists (appropriately normalized) into their
individual variables, `xGp90` and `xAp90`. 

```{r}
set.seed(9019)
km <- kmeans(select(nwsl_cluster_df, xGp90, xAp90) |> 
               na.omit(),
             centers = 3)

nwsl_cluster_df <- mutate(nwsl_cluster_df, clusters = km$cluster)
```


```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = xAp90, y = xGp90, col = as.factor(clusters)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
    labs(x = "Expected Assists per 90",
       y = "Expected Goals per 90")

```

How do you relate this to position players? We can change the color and/or shape
of the points based on position of the players. In this case for simplicity, I 
changed the original position variable in the subsetted data set to just three 
positions: defender (D), midfielder (M), and forward (F).

```{r}
p <- ggplot(data = nwsl_cluster_df |> 
              mutate(short_pos = substr(pos, 1, 1)),
            aes(x = xAp90, y = xGp90, shape = short_pos, col = as.factor(clusters)))
p + geom_point(alpha = .75) +
  scale_color_brewer("cluster", palette = "Dark2") +
  labs(shape = "position",
       x = "Expected Assists per 90",
       y = "Expected Goals per 90")

```

```{r}
p <- ggplot(data = nwsl_cluster_df |> 
              mutate(short_pos = substr(pos, 1, 1)),
            aes(x = xAp90, y = xGp90, col = short_pos, shape = as.factor(clusters)))
p + geom_point(alpha = .75) +
  scale_color_brewer("position", palette = "Dark2") +
  labs(shape = "cluster",
       x = "Expected Assists per 90",
       y = "Expected Goals per 90")

```

Do certain positions dominate certain clusters?

### Hierarchical Clustering

Hierarchical clustering methods are a commonly used class of alternatives to 
$k$-means clustering. As the name suggests, the methodology will produce a 
"hierarchy" amongst the observations relative to some distance measure, and the 
results are easy to visualize. There are several competing hierarchical methods
from which to choose, however, we will only focus on those that are agglomerative 
in nature. That is, each observation is considered its own cluster initially and 
are paired up in subsequent steps based on dissimilarity meausures between 
observations/clusters. There are several methods to perform the groups, e.g. 
single, average, median, etc. linkage. The default is to use a centroid linkage.

```{r}
nwsl_cluster_df <- nwsl_player_stats |> 
  mutate(player_team = paste(player, squad, sep = "_")) |> 
  select(player, squad, player_team, age, starts, xGp90, xAp90, xGxAp90, 
                npxGp90, npxGxAp90) |> 
  filter(xAp90 < 2) |> 
  na.omit()
row.names(nwsl_cluster_df) <- nwsl_cluster_df$player_team
hc <- hclust(dist(nwsl_cluster_df), "average")
```

The following figure shows that with a hierarchical clustering, we can ultimately
assign each observation to its own cluster. Obviously, this is not useful. We have
to cut the clustering at a certain level in order to make this a meaningful 
methodology. 

```{r}
p <- ggdendrogram(hc, labels = F, size = 2)
p
```

As you can see, it is extremely difficult to visualize the dendogram in its 
entirety. We can, however, visualize specific pieces and try to identify 
individuals within clusters. We can easily see how many elements fall into a 
particular clustering, e.g. five clusters or groups using the `cutree` function. 

```{r}
clusters <- cutree(hc, k = 5)
table(clusters)
```

Do we see any patters in the data with respect to clusters? 
```{r}
nwsl_cluster_df <- nwsl_cluster_df |> 
  mutate(clusters = clusters)
```

```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = starts, col = as.factor(clusters)))
p + geom_point(alpha = .75) +
  scale_color_brewer("cluster", palette = "Dark2") +
  labs(shape = "position",
       x = "Age of Players",
       y = "Number of Starts")
```
It looks like this clustering is dominated by the `age` and `starts` variabldes. 

### Gaussian Mixtures

This section is one of the more advanced topic in the textbook, however, it we
feel that it's utility in a host of differing domains is worth noting. The class
of algorithm that we will explore here is often referred to as Gaussian, or normal,
mixture models. 

The two preceding clustering algorithms result in a "hard" clustering of 
observations into groups as there is no room for ambiguity in the cluster 
assignments. We now turn our attention to a probability-based clustering 
algorithm based on mixtures of $K$ Gaussian distributions. The result is a "soft"
clustering in which each observation is assigned to a cluster using a probability. 
We write the probability of observation $\boldsymbol{x}_i$ belonging to cluster 
$k = 1, 2, \dots, K$ as $P(i^{th}\ \textrm{observation} \in k | x_i)$, with 
as 
$$
\sum_{k = 1}^KP(i^{th}\ \textrm{observation} \in k | x_i) = 1.
$$
The Gaussian (Normal) mixture model can be written as 
$$
f(\boldsymbol{x}) = \sum_{k = 1}^K\pi_kf(\boldsymbol{x}; \boldsymbol{\mu}_k, \Sigma_k)
$$ 
where each $f(\boldsymbol{x}; \boldsymbol{\mu}_k, \Sigma_k)$ is itself a multivariate
Gaussian distribution characterized by its mean, $\boldsymbol{\mu}$ and covariance
matrix, $\Sigma_k$. An often-used simplying assumption is that each component shares
a common covariance matrix, say $\Sigma_k = \Sigma$. The individual mixing proportions 
$\pi_k$ represent the probability of an individual observational unit belonging to the 
$k^{th}$ cluster with $\sum_{k = 1}^K\pi_k = 1$, i.e. every unit belongs to one of 
the clusters. 

Although the theoretical details of Gaussian mixture models are well beyond the 
scope of this text, the implementation details are relatively straightforward. 
We will use the `mixtools` R package to fit a Gaussian mixture model using an 
Expectation-Maximization (EM) algorithm and making cluster assignments. We will 
start with a two-component, or two-cluster, mixture using the expected goals and 
expected assists variables. In order to initialize the algorithm, you must specify 
{\em a priori} the mean vector and covariance matrix of the individual components, 
as well as the initial mixing probabilities. We assume a common covariance matrix 
in our initial example given next. Further, we take $\boldsymbol{\mu}_1 = c(21, 0.1)^\prime$,
$\boldsymbol{\mu}_2 = c(30, 0.5)^\prime$, $\pi_1 = 0.15$, and $\pi_2 = 0.85$. 


```{r}
set.seed(109823)
nwsl_mix_df <- nwsl_cluster_df |> 
  filter(xGxAp90 < 2)
mix_model <- mvnormalmixEM(nwsl_mix_df[, c("age", "xGxAp90")], 
                           arbvar = FALSE,
                           mu = list(c(21, .1), c(30, .5)),
                           lambda = c(.15, .85),
                           k = 2,
                           epsilon = 1e-02,
                           verb = T)

```
The individual observations are clustered according to their estimated posterior 
probabilities of cluster assignment. The `mix_model` model object stores these estimates 
in the `posterior` object. Look at the first six values in this list.
```{r}
head(mix_model$posterior)
```

The first line in the `posterior` output shows 0.00003 and 0.99997 for `comp.1` and
`comp.2`, respectively. These numbers represent the posterior probability of 
belonging to the first and second components, or clusters. In particular, the model 
classifies someone having `age = 23` and `xGxAp90 = 0` as belonging to the second 
cluster given that the first cluster's posterior probability is lower than the 
second. On the other hand, someone with `age = 22` and `xGxAp90 = 0.62` is 
assigned to the first cluster. As mentioned above, this is referred to as a 
"soft" clustering given that we are assigning group member ship based on the
posterior probabilities. 


```{r}
head(nwsl_mix_df[, c("age", "xGxAp90")])
```

```{r}
mix_model$mu
```

Let's add the centers to our scatterplot and color the points according to their 
classification. The first variable (`pp`) would be our soft classification 
probability whereas the `class` variable is a hard classification. We can 
visualize both using some of the tools that we have used previously. The centers, 
or means, of the individual cluster distributions, $f_k$, are given as the solid
black triangles in the following figure. 

```{r}
nwsl_mix_df <- nwsl_mix_df |> 
  mutate(pp = mix_model$posterior[, 1],
                class = ifelse(pp <= .5, "one", "two"))

```


```{r}
p <- ggplot(data = nwsl_mix_df,
            aes(x = age, y = xGxAp90, col = class))
p + geom_point() +
  geom_point(data = data.frame(age = c(mix_model$mu[[1]][1], mix_model$mu[[2]][1]),
                               xGxAp90 = c(mix_model$mu[[1]][2], mix_model$mu[[2]][2])),
             aes(x = age, y = xGxAp90), col = "black", shape = 17, size = 3) +
  scale_color_brewer(palette = "Set1") +
  labs(color = "cluster") +
  theme_bw()
```

```{r}
p <- ggplot(data = nwsl_mix_df,
            aes(x = age, y = xGxAp90, col = pp))
p + geom_point() +
  geom_point(data = data.frame(age = c(mix_model$mu[[1]][1], mix_model$mu[[2]][1]),
                               xGxAp90 = c(mix_model$mu[[1]][2], mix_model$mu[[2]][2])),
             aes(x = age, y = xGxAp90), col = "black", shape = 17, size = 3) +

  scale_color_distiller("Probability", palette = "Set1", limits = c(0, 1)) +
  labs(color = "cluster",
       x = "Age",
       y = "Expected Goals and Assists per 90") +
  theme_bw()
```
We `lambda` parameter in the `mvnormalmixEM` object yields the estimated mixing 
proportions for each component. In this example, we have $\hat{\pi}_1 = $ 
`r mix_model$lambda[1]` and $\hat{\pi}_2 = $ `r mix_model$lambda[2]`.

```{r}
mix_model$lambda
```

One thing to note about using the `mixtools` specifically or EM algorithms in 
general is that you should always initialize the algorithm using multiple inital 
values in order to ensure convergence to a global maximum. You can check the 
log-likelihood in the output to see if you (hopefully) arrive at a global maximum. 
Consider a different initialization of the same function call and examine the 
log-likelihood. 

```{r}
set.seed(109823)
mix_model_2 <- mvnormalmixEM(nwsl_mix_df[, c("age", "xGxAp90")], 
                             arbvar = FALSE,
                             mu = list(c(25, .1), c(20, .5)),
                             lambda = c(.5, .5),
                             k = 2,
                             epsilon = 1e-02,
                             verb = T)

```

```{r}
mix_model$loglik
```

```{r}
mix_model_2$loglik
```

## Exercises

Problems 1 - 5 of the exercises will use `epl_team_stats` data set in the 
author's `ISAR` R package. Exercise six will use 

1. Plot average age of a team age (`age`) versus expected goals and assists per 
90 minutes (`xG_xA_p90`). 

```{r}
p <- ggplot(data = epl_team_stats,
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  theme_bw()
```

2. Add a smooth trend line to the plot given in (1) and comment on your 
observations.

```{r}
p <- ggplot(data = epl_team_stats,
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  geom_smooth(color = "navy") +
  theme_bw()
```

3. Label all points that are above three in `xG_xA_p90` by team and season.

```{r}
p <- ggplot(data = epl_team_stats |> 
              mutate(team_season = ifelse(xG_xA_p90 > 3.5,
                                                 paste(squad, season, sep = " "),
                                                 NA)),
            aes(x = age, y = xG_xA_p90, label = team_season))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  geom_smooth(color = "navy") +
  geom_text_repel() +
  theme_bw()
```

4. Remove Manchester City and redo the trend plot. Add a linear trend line to the 
plot as well. 

```{r}
p <- ggplot(data = epl_team_stats |> 
              filter(squad != "Manchester City"),
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  geom_smooth(method = "lm", color = "navy") +
  geom_smooth() +
  theme_bw()

```

5. Repeat problems (1) - (2) using data from the four most recent season and facet
by season. 

```{r}
p <- ggplot(data = epl_team_stats |> 
              filter(season != "2017-18"),
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  facet_wrap(~ season, nc = 2) +
  theme_bw()

```
```{r}
p <- ggplot(data = epl_team_stats |> 
              filter(season != "2017-18"),
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  facet_wrap(~ season, nc = 2, scales = "free_x") +
  geom_smooth(color = "navy") +
  theme_bw()

```

6. Using the `epl_player_stats_2022` data frame from the `ISAR` package, plot 
expected goals plus assists per 90 (`xG_xA_p90`) on the y axis and minutes 
played (`min`) on the y. Label the players having an `xG_xA_p90` value greater
than one.

```{r}
p <- ggplot(data = epl_player_stats_2022 |> 
              mutate(new_player = ifelse(xG_xA_p90 > 1,
                                                player,
                                                NA)),
            aes(x = min, y = xG_xA_p90, label = new_player))
p + geom_point(alpha = .5) + 
  geom_smooth(method = "lm") +
  labs(x = "Minutes", y = "Expected goals and assists per 90 minutes") +
  geom_text_repel()
```

7. Filter the data set to players who played more than 90 minutes. Regress 
expected goals plus assists per 90 (`xG_xA_p90`) on minutes played (`min`). Does 
the result surprise you? 

```{r}
epl_lm <- lm(xG_xA_p90 ~ min, data = epl_player_stats_2022 |> 
               filter(min > 90))
summary(epl_lm)
```
The insignificant slope parameter should not be a surprise given that the 
dependent variable has been normalized based on a per 90 minute criterion. 

8. Redo problems (6) and (7) using expected goals plus expected assists as the 
dependent variable.  

```{r}
p <- ggplot(data = epl_player_stats_2022 |> 
               filter(min > 90) |> 
               mutate(xG_xA = xG + xA),
            aes(x = min, y = xG_xA))
p + geom_point(alpha = .5) + 
  geom_smooth(method = "lm") +
  labs(x = "Minutes", y = "Expected goals and assists per 90 minutes") 
# +
#   geom_text_repel()
```
```{r}
epl_lm <- lm(xG_xA ~ min, data = epl_player_stats_2022 |> 
               filter(min > 90) |> 
               mutate(xG_xA = xG + xA))
summary(epl_lm)
```

9. Cluster analysis based on EPL data: `epl_stats_2022` from R package

## Case Study
We will use the `worldfootballR` package to match player valuations based on 
[Transfermarkt.com](https://www.transfermarkt.com). In particular, use the 
`worldfootballR::tm_player_market_values()` function to pull all EPL valuations
for players in the 2021 - 2022 season and join these valuations with the `ISAR`
package's `epl_player_stats_2022` data. Perform a full accounting of valuations in 
order to identify under- and over-paid players. 

```{r, eval = F}
epl_valuations <- tm_player_market_values(country_name = "England",
                                          start_year = 2021)
```

Join with our `epl_player_stats_2022` data from 
```{r, eval = F}
epl_stats_val <- left_join(epl_player_stats_2022, epl_valuations,
                                  by = c("player" = "player_name"))
```


## Lab
In this lab, we will investigate playing styles by goalkeepers in the English 
Premier League. The data for the lab can be found in the `ISAR` package under
`epl_gk_stats_2022`. The data set consists of 42 observations (goalkeepers) across 
32 variables. We will perform a k-means clustering of the players to gain 
insights into their shot stopping and passing styles. 

1. Create a k-means clustering of goalkeepers based on the following variables: 
post shot expected goals per shot on target (`post_shot_xG_per_shot_on_target`), 
post shot expected goals minus goals scored per 90 minutes 
(`post_shot_xG_minus_goals_p90`), launch completion percent 
(`launch_completion_percent`), pass launch percentage (`pass_launch_percent`), 
average pass length (`avg_pass_len`), 
goal kick launch percent (`gk_launch_percent`), and defensive actions outside the
penalty area per 90 minutes (`def_actions_out_pen_area_p90`). Set your seed at
8675.

```{r}
set.seed(8675)
epl_gk_kmean <- kmeans(select(epl_gk_stats_2022, 
                                     post_shot_xG_per_shot_on_target, 
                                     launch_completion_percent, 
                                     pass_launch_percent, 
                                     avg_pass_len,
                                     gk_launch_percent,
                                     def_actions_out_pen_area_p90) |> 
                         na.omit(),
                       centers = 2)
```

```{r}
epl_gk_kmean
```
2. Create a data frame showing the players, their nationality, and their cluster
assignment. Sort the data by cluster. 

```{r}
epl_gk_stats_2022 |>mutate(k_means = epl_gk_kmean$cluster) |> 
  select(player, nation, k_means) |> 
  arrange(k_means)
```

3. Plot the average pass length by the number of defensive actions outside the 
penalty area per ninety minutes. Color the points by the cluster membership. 
```{r}
epl_cluster_df <- mutate(epl_gk_stats_2022, 
                                k_means = epl_gk_kmean$cluster) 

p <- ggplot(data = epl_cluster_df,
            aes(x = avg_pass_len, 
                y = def_actions_out_pen_area_p90, 
                col = as.factor(k_means)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Set1") +
  labs(x = "Average Pass Length",
       y = "Defensive Actions Outside the Penalty Area per 90")
```
4. Add simple linear regression trend lines for each cluster. What can you infer 
based on these trend lines. 

```{r}
p <- ggplot(data = epl_cluster_df,
            aes(x = avg_pass_len, 
                y = def_actions_out_pen_area_p90, 
                col = as.factor(k_means)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Set1") +
  labs(x = "Average Pass Length",
       y = "Defensive Actions Outside the Penalty Area per 90") +
  geom_smooth(method = "lm")

```

