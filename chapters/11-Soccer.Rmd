---
title: "Chapter XXX -- Soccer"
author: "Ryan Elmore"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r knitr-options, include = FALSE}
knitr::opts_chunk$set(fig.align="center",
                      warning = FALSE,
                      message = FALSE,
                      comment = NA)
```

```{r, eval=F, include = F}
remotes::install_github("rtelmore/ISAR")
```

```{r libs, include=T, message = F, warning = F}
library(dplyr)
library(ggplot2)
library(lubridate)
library(rvest)
library(janitor)
library(scales)
library(readr)
library(nhlapi)
library(purrr)
library(sportyR)
library(ggrepel)
library(ggdendro)
library(ISAR)
library(mixtools)
theme_set(theme_bw())
```

# Soccer Data

In this chapter, we will explore several unsupervised learning methods, e.g. 
hierarchical clustering, using several hockey data sets. 

## FB Reference 

In October 2022, the Sports Reference website announced a partnership with 
Stats Perform Opta to dramatically increase the quantity and quality of soccer, 
or football, data that are available on its site. Soccer data are now hosted at
https://fbref.com/en/ and includes data on most major professional leagues 
throughout the world at a team and player level. More details related to this 
new partnership can be found here. 

https://www.sports-reference.com/blog/2022/10/fbref-leagues-ðŸ‡µðŸ‡¹-leagues-ðŸ‡§ðŸ‡·-leagues-ðŸ‡²ðŸ‡½-expanded-womens-and-mens-data-new-data-partner/

In this chapter, we will focus on using soccer data to cluster both teams and 
individual players using data from fbref.com. 

## Expected Goals and Assists

Similar to hockey, soccer is notoriously low-scoring and there is 
certainly an element of luck in scoring a goal. Therefore, one of the main 
developments in the soccer-analytics space is the introduction of Expected Goals, 
or xG for short. There are various models to compute xG, but at a high level xG is
simply the probability of scoring a goal from a given spot on the field when a 
shot is taken.

Given that most shots do not result in a goal, most passes to a shot taker will 
likely lead to a missed goal as well. Therefore, there is an analogous result for 
assists that is referred to as Expected Assists, or xA for short. You will often
see these metrics normalized to ninety minutes so that a player is a substitute 
and only plays thirty minutes can be directly compared to a starter who might 
play sixty - ninety minutes. The normalized expected goals and assists are often
written as xG (xA) per ninety. 

## Player Statistics

We will initially focus our attention on clustering individual players from the 
American NWSL. The NWSL is National Women's Soccer League and is the largest
women's soccer league in North America. In the 2022 season there were twelve teams
and the Portland Thorns were crowned champion. 

We downloaded a csv file of the NWSL player statistics
from the following fbref.com website and the data are also given in the book's R 
package as `nwsl_player_stats`. Note that the column, or variable, names are defined 
on the Football Reference website. https://fbref.com/en/comps/182/stats/NWSL-Stats


Here's how the data were obtained. Note, they are now located in the `ISAR` package
under the `nwsl_player_stats` object. 

```{r, eval = F}
nwsl_player_stats <- read.csv("../data/nwsl-players.csv", header = T) |> 
  janitor::clean_names() |> 
  dplyr::select(c(2:6, 8:10, 28:32)) |> 
  dplyr::rename(xGp90 = x_g_1,
                xAp90 = x_ag_1,
                xGxAp90 = x_g_x_ag,
                npxGp90 = npx_g_1,
                npxGxAp90 = npx_g_x_ag_1)
saveRDS(nwsl_player_stats, "../data/nwsl_player_stats.rds")
```

### Exploratory Data Analysis

We will begin by exploring expected goals. In particular, let's look at xG by 
age in the NWSL. We will use the `xGp90` column as our xG value because it is 
the column representing the normalized xG value, normalizing xG per 90 minutes 
played. In addition, we restrict our analysis to the non-goalkeeping position
players as the goalkeepers are not likely to score or assist on a goal. 

```{r}
p <- ggplot(data = nwsl_player_stats |> 
              dplyr::filter(pos != "GK"),
            aes(x = age, y = xGp90))
p + geom_point(alpha = .5) +
  labs(x = "age", y = "expected goals (xG)") +
  theme_bw()
```
One thing that we might want to do in plotting sports data is to identify the 
players that we are plotting. In this case, we can use the `ggrepel` package to 
quickly add labels to certain points using the `label` aesthetic and the
`geom_text_repel()` function. Suppose we want to identify players who are less 
than 20 years of age with a positive xG value, or have an xG value greater than 
0.6. 

```{r}
p <- ggplot(data = nwsl_player_stats |> 
              dplyr::filter(pos != "GK") |> 
              dplyr::mutate(new_player = ifelse((age < 20 & xGp90 > 0) | xGp90 > 0.6,
                            player, "")),
            aes(x = age, y = xGp90, label = new_player))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals (xG)") +
  geom_text_repel() +
  theme_bw()
```

## Clustering

In this chapter we will focus on several analytical tools that fall under the  
statistical, or machine, learning umbrella of unsupervised learning. Unsupervised
learning is generally thought to be exploratory data analytic techniques that 
are designed to learn as much as possible about the data without the aid of 
labels as is the case in a regression or classification model (supervised 
learning). Clustering, in particular, refers to the class of techniques concerned
with finding groups within your data set that are similar (in some sense) with 
respect to the measured variables (or covariates). 

### $K$-Means

In short, $K$-means clustering refers to assigning each observation to one of 
$K$ clusters. The number of clusters, $K$ is specified *a priori*. The algorithm
for computing a $K$-means clustering of $n$ observations is relatively simple 
and intuitive. Each observation is randomly assigned to one of the $K$ clusters
and the "center" of each cluster is computed, e.g. the mean vector in $K$-means. 
Next, you assign each observation to the closest "center", recompute the "center", 
and reassign each observation until observations stop being reassigned. 

#### Clustering with Age

First let's look at a clustering based on age of the player (age), total minutes 
played in the season (min), and expected goals and assists per ninety minutes 
played (xGxAp90). We are going to consider three clusters initially. 

```{r}
nwsl_cluster_df <- nwsl_player_stats |> 
  dplyr::mutate(player_team = paste(player, squad, sep = "_")) |> 
  dplyr::select(player, squad, player_team, pos, age, min, starts, xGp90, xAp90, 
                xGxAp90, npxGp90, npxGxAp90) |> 
  dplyr::filter(xGxAp90 < 1.5, pos != "GK") |> 
  na.omit()

set.seed(9019)
km <- kmeans(dplyr::select(nwsl_cluster_df, age, min, xGp90, xAp90) |> 
               na.omit(),
             centers = 3)
```

How can we visualize these clusters? Let's plot age versus expected goals and 
assists, and color these points by their clusters. 

```{r}
nwsl_cluster_df <- dplyr::mutate(nwsl_cluster_df, k_means = km$cluster)

p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = min, col = as.factor(k_means)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
  labs(x = "Age of Player",
       y = "Minutes Played")
```

What stands out amongst this clustering of points? Clearly the clusters are 
dominated by different age strata. It looks like the first cluster is composed
of players that played more than approximately 1200 minutes, the second contains
players playing (roughly) 500 to 1200 minutes, and finally the last cluster 
contains players playing less than 500 minutes. 

Is anything else evident if we omit minutes played and plot age of the player 
versus expected goals and assists? 

```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = xGxAp90, col = as.factor(k_means)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
  labs(x = "Age of Player",
       y = "Expected Goals and Assists per 90")
  

```

#### Remove Minutes Played

Given that minutes played dominates the previous clustering, consider a 
$K$-means clustering age, expected goals, and expected assists, both normalized
by per ninety minutes played. 

```{r}
set.seed(9019)
km <- kmeans(dplyr::select(nwsl_cluster_df, age, xGp90, xAp90) |> 
               na.omit(),
             centers = 3)

nwsl_cluster_df <- dplyr::mutate(nwsl_cluster_df, clusters = km$cluster)
```

```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = xGp90, col = as.factor(clusters)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
    labs(x = "Age of Player",
       y = "Expected Goals per 90")

```

As you might imagine, the clustering is now dominated by the age variable.  


```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = age, y = xAp90, col = as.factor(clusters)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
    labs(x = "Age of Player",
       y = "Expected Assists per 90")

```

It's obvious that by including `age` and `minutes` seems to dominate
the makeup of the clusters. So, if we want to identify similar players 
based on their scoring ability, let's only consider the appropriate metrics. Here
we separate the expected goals and assists (appropriately normalized) into their
individual variables, `xGp90` and `xAp90`. 

```{r}
set.seed(9019)
km <- kmeans(dplyr::select(nwsl_cluster_df, xGp90, xAp90) |> 
               na.omit(),
             centers = 3)

nwsl_cluster_df <- dplyr::mutate(nwsl_cluster_df, clusters = km$cluster)
```


```{r}
p <- ggplot(data = nwsl_cluster_df,
            aes(x = xAp90, y = xGp90, col = as.factor(clusters)))
p + geom_point() +
  scale_color_brewer("cluster", palette = "Dark2") +
    labs(x = "Expected Assists per 90",
       y = "Expected Goals per 90")

```

How do you relate this to position players? We can change the color and/or shape
of the points based on position of the players. In this case for simplicity, I 
changed the original position variable in the subsetted data set to just three 
positions: defender (D), midfielder (M), and forward (F).

```{r}
p <- ggplot(data = nwsl_cluster_df |> 
              dplyr::mutate(short_pos = substr(pos, 1, 1)),
            aes(x = xAp90, y = xGp90, shape = short_pos, col = as.factor(clusters)))
p + geom_point(alpha = .75) +
  scale_color_brewer("cluster", palette = "Dark2") +
  labs(shape = "position",
       x = "Expected Assists per 90",
       y = "Expected Goals per 90")

```

```{r}
p <- ggplot(data = nwsl_cluster_df |> 
              dplyr::mutate(short_pos = substr(pos, 1, 1)),
            aes(x = xAp90, y = xGp90, col = short_pos, shape = as.factor(clusters)))
p + geom_point(alpha = .75) +
  scale_color_brewer("position", palette = "Dark2") +
  labs(shape = "cluster",
       x = "Expected Assists per 90",
       y = "Expected Goals per 90")

```

Do certain positions dominate certain clusters?

### Hierarchical Clustering

```{r}
nwsl_cluster_df <- nwsl_player_stats |> 
  dplyr::mutate(player_team = paste(player, squad, sep = "_")) |> 
  dplyr::select(player, squad, player_team, age, starts, xGp90, xAp90, xGxAp90, 
                npxGp90, npxGxAp90) |> 
  na.omit()
row.names(nwsl_cluster_df) <- nwsl_cluster_df$player_team
hc <- hclust(dist(nwsl_cluster_df), "average")
p <- ggdendrogram(hc, rotate = FALSE, size = 2)
p
```


### Gaussian Mixtures

The two preceding clustering algorithms result in a "hard" clustering of 
observations into groups as there is no room for ambiguity in the cluster 
assignments. We now turn our attention to a probability-based clustering 
algorithm based on mixtures of $K$ Gaussian distributions. The result is a "soft"
clustering in which each observation is assigned to a cluster using a probability. 
We write the probability of observation $\boldsymbol{x}_i$ belonging to cluster 
$k = 1, 2, \dots, K$ as $P(i^{th}\ \textrm{observation} \in k | x_i)$, with 
as 
$$
\sum_{k = 1}^KP(i^{th}\ \textrm{observation} \in k | x_i) = 1.
$$
The Gaussian (Normal) mixture model can be written as 
$$
f(\boldsymbol{x}) = \sum_{k = 1}^K\pi_kf(\boldsymbol{x}; \boldsymbol{\mu}_k, \Sigma_k)
$$ 
where each $f(\boldsymbol{x}; \boldsymbol{\mu}_k, \Sigma_k)$ is itself a multivariate
Gaussian distribution characterized by its mean, $\boldsymbol{\mu}$ and covariance
matrix, $\Sigma_k$. An often-used simplying assumption is that each component shares
a common covariance matrix, say $\Sigma_k = \Sigma$. The individual mixing proportions 
$\pi_k$ represent the probability of an individual observational unit belonging to the 
$k^{th}$ cluster with $\sum_{k = 1}^K\pi_k = 1$, i.e. every unit belongs to one of 
the clusters. 

Although the theoretical details of Gaussian mixture models are well beyond the 
scope of this text, the implementation details are relatively straightforward. 
We will use the `mixtools` R package to fit a Gaussian mixture model using an 
Expectation-Maximization (EM) algorithm and making cluster assignments. We will 
start with a two-component, or two-cluster, mixture using the expected goals and 
expected assists variables. In order to initialize the algorithm, you must specify 
{\em a priori} the mean vector and covariance matrix of the individual components, 
as well as the initial mixing probabilities. We assume a common covariance matrix 
in our initial example given next. Further, we take $\boldsymbol{\mu}_1 = c(21, 0.1)^\prime$,
$\boldsymbol{\mu}_2 = c(30, 0.5)^\prime$, $\pi_1 = 0.15$, and $\pi_2 = 0.85$. 


```{r}
set.seed(109823)
nwsl_mix_df <- nwsl_cluster_df |> 
  dplyr::filter(xGxAp90 < 2)
mix_model <- mvnormalmixEM(nwsl_mix_df[, c("age", "xGxAp90")], 
                           arbvar = FALSE,
                           mu = list(c(21, .1), c(30, .5)),
                           lambda = c(.15, .85),
                           k = 2,
                           epsilon = 1e-02,
                           verb = T)

```
The individual observations are clustered according to their estimated posterior 
probabilities of cluster assignment. The `mix_model` model object stores these estimates 
in the `posterior` object. Look at the first six values in this list.
```{r}
head(mix_model$posterior)
```

The first line in the `posterior` output shows 0.00005 and 0.99995 for `comp.1` and
`comp.2`, respectively. These numbers represent the posterior probability of 
belonging to the first and second components, or clusters. In particular, the model 
classifies someone having `age = 23` and `xGxAp90 = 0` as belonging to the second 
cluster given that the first cluster's posterior probability is lower than the 
second. On the other hand, someone with `age = 22` and `xGxAp90 = 0.62` is 
assigned to the first cluster. As mentioned above, this is referred to as a 
"soft" clustering given that we are assigning group member ship based on the
posterior probabilities. 


```{r}
head(nwsl_mix_df[, c("age", "xGxAp90")])
```

```{r}
mix_model$mu
```

Let's add the centers to our scatterplot and color the points according to their 
classification. The first variable (`pp`) would be our soft classification 
probability whereas the `class` variable is a hard classification. We can 
visualize both using some of the tools that we have used previously. The centers, 
or means, of the individual cluster distributions, $f_k$, are given as the solid
black triangles in the following figure. 

```{r}
nwsl_mix_df <- nwsl_mix_df |> 
  dplyr::mutate(pp = mix_model$posterior[, 1],
                class = ifelse(pp <= .5, "one", "two"))

```


```{r}
p <- ggplot(data = nwsl_mix_df,
            aes(x = age, y = xGxAp90, col = class))
p + geom_point() +
  geom_point(data = data.frame(age = c(mix_model$mu[[1]][1], mix_model$mu[[2]][1]),
                               xGxAp90 = c(mix_model$mu[[1]][2], mix_model$mu[[2]][2])),
             aes(x = age, y = xGxAp90), col = "black", shape = 17, size = 3) +
  scale_color_brewer(palette = "Set1") +
  labs(color = "cluster") +
  theme_bw()
```

```{r}
p <- ggplot(data = nwsl_mix_df,
            aes(x = age, y = xGxAp90, col = pp))
p + geom_point() +
  geom_point(data = data.frame(age = c(mix_model$mu[[1]][1], mix_model$mu[[2]][1]),
                               xGxAp90 = c(mix_model$mu[[1]][2], mix_model$mu[[2]][2])),
             aes(x = age, y = xGxAp90), col = "black", shape = 17, size = 3) +

  scale_color_distiller("Probability", palette = "Set1", limits = c(0, 1)) +
  labs(color = "cluster",
       x = "Age",
       y = "Expected Goals and Assists per 90") +
  theme_bw()
```
We `lambda` parameter in the `mvnormalmixEM` object yields the estimated mixing 
proportions for each component. In this example, we have $\hat{\pi}_1 = $ 
`r mix_model$lambda[1]` and $\hat{\pi}_2 = $ `r mix_model$lambda[2]`.

```{r}
mix_model$lambda
```

One thing to note about using the `mixtools` specifically or EM algorithms in 
general is that you should always initialize the algorithm using multiple inital 
values in order to ensure convergence to a global maximum. You can check the 
log-likelihood in the output to see if you (hopefully) arrive at a global maximum. 
Consider a different initialization of the same function call and examine the 
log-likelihood. 

```{r}
set.seed(109823)
mix_model_2 <- mvnormalmixEM(nwsl_mix_df[, c("age", "xGxAp90")], 
                             arbvar = FALSE,
                             mu = list(c(25, .1), c(20, .5)),
                             lambda = c(.5, .5),
                             k = 2,
                             epsilon = 1e-02,
                             verb = T)

```

```{r}
mix_model$loglik
```

```{r}
mix_model_2$loglik
```

## Exercises

Most of the exercises will use `epl_team_stats` data set in the author's `ISAR` 
R package. 

1. Plot average age of a team age (`age`) versus expected goals and assists per 
90 minutes (`xG_xA_p90`). 

```{r}
p <- ggplot(data = epl_team_stats,
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  theme_bw()
```

2. Add a smooth trend line to the plot given in (1) and comment on your 
observations.

```{r}
p <- ggplot(data = epl_team_stats,
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  geom_smooth(color = "navy") +
  theme_bw()
```

3. Label all points that are above three in `xG_xA_p90` by team and season.

```{r}
p <- ggplot(data = epl_team_stats |> 
              dplyr::mutate(team_season = ifelse(xG_xA_p90 > 3.5,
                                                 paste(squad, season, sep = " "),
                                                 NA)),
            aes(x = age, y = xG_xA_p90, label = team_season))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  geom_smooth(color = "navy") +
  geom_text_repel() +
  theme_bw()
```

4. Remove Manchester City and redo the trend plot. Add a linear trend line to the 
plot as well. 

```{r}
p <- ggplot(data = epl_team_stats |> 
              dplyr::filter(squad != "Manchester City"),
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  geom_smooth(method = "lm", color = "navy") +
  geom_smooth() +
  theme_bw()

```

5. Repeat problems (1) - (2) using data from the four most recent season and facet
by season. 

```{r}
p <- ggplot(data = epl_team_stats |> 
              dplyr::filter(season != "2017-18"),
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  facet_wrap(~ season, nc = 2) +
  theme_bw()

```
```{r}
p <- ggplot(data = epl_team_stats |> 
              dplyr::filter(season != "2017-18"),
            aes(x = age, y = xG_xA_p90))
p + geom_point(alpha = .5) +
  labs(x = "Age", y = "Expected goals and assists per 90 minutes") +
  facet_wrap(~ season, nc = 2, scales = "free_x") +
  geom_smooth(color = "navy") +
  theme_bw()

```
Faceted density plots of xG, xA by position

2. Regress xGp90 and xAp90 on min to see if someone is not playing as much as 
you might think they should play. 
3. Cluster analysis based on EPL data: `epl_stats_2022` from R package

## Case Study
Something related to [Transfermarkt.com](https://www.transfermarkt.com).

## Lab
EPL Goalkeeper study using `epl_gk_stats_2022` from R package.



